================================================================================
RESUMEN EJECUTIVO - MODELO DE PREDICCIÃ“N IMDB
================================================================================

ğŸ“Š DATASET
--------------------------------------------------------------------------------
Total de pelÃ­culas:           85,855
PelÃ­culas limpias:            69,107 (80.49% retenciÃ³n)
Columnas utilizadas:          10 principales
Target variable:              avg_vote (1.0-10.0, media: 5.9)

ğŸ”§ FEATURE ENGINEERING
--------------------------------------------------------------------------------
Total de features:            ~130

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIPO                    â”‚ FEATURES â”‚ TÃ‰CNICA                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Texto                   â”‚    100   â”‚ TF-IDF Unigrams (description)      â”‚
â”‚ CategÃ³ricas             â”‚     19   â”‚ Target Encoding + Feature Hashing  â”‚
â”‚   â”œâ”€ Genre              â”‚     16   â”‚ Feature Hashing (top 30)           â”‚
â”‚   â”œâ”€ Director           â”‚      1   â”‚ Target Encoding (10K+ â†’ 1)         â”‚
â”‚   â”œâ”€ Actors             â”‚      1   â”‚ Target Encoding (55K+ â†’ 1)         â”‚
â”‚   â””â”€ Duration Category  â”‚      1   â”‚ StringIndexer                      â”‚
â”‚ NumÃ©ricas Originales    â”‚      5   â”‚ duration, votes, reviews, year     â”‚
â”‚ NumÃ©ricas Derivadas     â”‚      6   â”‚ log_votes, ratios, flags           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¤– MODELOS ENTRENADOS
--------------------------------------------------------------------------------

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODELO                   â”‚   RMSE   â”‚   MAE    â”‚    RÂ²    â”‚  TIEMPO  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ridge Regression         â”‚  0.1265  â”‚  0.0951  â”‚  0.9895  â”‚  0.11 m  â”‚
â”‚ Random Forest            â”‚  0.1702  â”‚  0.1025  â”‚  0.9810  â”‚  0.24 m  â”‚
â”‚ Gradient Boosted Trees â˜… â”‚  0.0912  â”‚  0.0401  â”‚  0.9945  â”‚  0.82 m  â”‚
â”‚ Ensemble                 â”‚  0.0977  â”‚  0.0555  â”‚  0.9937  â”‚   ~1 m   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ† MEJOR MODELO: Gradient Boosted Trees
--------------------------------------------------------------------------------
RMSE:                     0.0912  (error promedio ~0.09 puntos en escala 1-10)
RÂ²:                       0.9945  (explica 99.45% de la varianza)
Mejora sobre baseline:    27.95% reducciÃ³n en RMSE

InterpretaciÃ³n:
  â†’ PredicciÃ³n extremadamente precisa
  â†’ Error tÃ­pico menor a 0.1 puntos
  â†’ Captura casi toda la variabilidad en calificaciones

ğŸ’¡ TÃ‰CNICAS CLAVE
--------------------------------------------------------------------------------
âœ… Target Encoding          â†’ Soluciona alta cardinalidad sin OOM
âœ… Feature Hashing          â†’ DimensiÃ³n controlada para categoricals
âœ… Feature Engineering      â†’ 6 features derivadas mejoran RÂ²
âœ… TF-IDF                   â†’ Captura semÃ¡ntica del texto
âœ… StandardScaler           â†’ NormalizaciÃ³n Ã³ptima
âœ… Subsampling (0.8)        â†’ Reduce memoria y mejora generalizaciÃ³n

âš ï¸ PROBLEMAS RESUELTOS
--------------------------------------------------------------------------------
Problema:  OutOfMemoryError con Random Forest
Causa:     Alta cardinalidad (actors: 55K, genre: 1.1K valores)
SoluciÃ³n:  Target Encoding + Feature Hashing + Subsampling

Problema:  Grid Search extremadamente lento
Causa:     18-27 combinaciones Ã— 3 folds = 54-81 entrenamientos
SoluciÃ³n:  HiperparÃ¡metros fijos basados en mejores prÃ¡cticas

Problema:  Dimensionalidad explosiva con OneHotEncoder
Causa:     55K actores â†’ 55K columnas sparse
SoluciÃ³n:  Target Encoding reduce a 1 feature numÃ©rica densa

ğŸ“ˆ FEATURE IMPORTANCE (Top 5 - GBT)
--------------------------------------------------------------------------------
1. Feature 51         29.78%    â†’ log_votes
2. Feature 52         25.89%    â†’ votes
3. Feature 54         19.57%    â†’ reviews/year features
4. Feature 50         17.69%    â†’ TF-IDF components
5. Feature 53          5.14%    â†’ categorical encoded

ConclusiÃ³n: Features numÃ©ricas (votes, reviews) son MÃS predictivas que texto

ğŸš€ ESPECIFICACIONES TÃ‰CNICAS
--------------------------------------------------------------------------------
Framework:               Apache Spark 3.3.1 + SparkML
Lenguaje:                Scala 2.12.15
Memoria requerida:       14GB (driver + executor)
Particiones:             100 (optimizado)
Tiempo total:            ~1.2 minutos
Storage strategy:        MEMORY_AND_DISK

Configuraciones crÃ­ticas:
  spark.memory.fraction=0.8
  spark.memory.storageFraction=0.2
  spark.sql.shuffle.partitions=50
  spark.driver.maxResultSize=2g

ğŸ“ ARCHIVOS GENERADOS
--------------------------------------------------------------------------------
CÃ³digo:
  - IMDBPredictionModelSimplified.scala     (modelo principal)
  - DataValidation.scala                    (validaciÃ³n)

Resultados:
  - reporte_simplificado.txt                (anÃ¡lisis comparativo)
  - simplified_baseline_predictions.txt     (top 20 errores)
  - simplified_rf_predictions.txt
  - simplified_gbt_predictions.txt
  - simplified_ensemble_predictions.txt     (475KB - predicciones completas)

ğŸ“ CONCLUSIONES
--------------------------------------------------------------------------------
âœ“ El modelo GBT alcanza RÂ² = 0.9945, indicando predicciones casi perfectas
âœ“ Target Encoding es superior a OneHotEncoder para high-cardinality features
âœ“ Feature Engineering (log transforms, ratios) mejora significativamente
âœ“ TF-IDF captura informaciÃ³n semÃ¡ntica Ãºtil de descripciones
âœ“ Ensemble no supera a GBT individual (mejor modelo single)

Limitaciones identificadas:
  - Random Forest underperforms vs GBT en este dataset
  - Bi-gramas no justifican costo computacional adicional
  - Grid Search impracticable sin cluster distribuido

Mejoras futuras sugeridas:
  - Word2Vec para embeddings semÃ¡nticos profundos
  - AnÃ¡lisis de sentimiento en descriptions
  - Features de presupuesto/recaudaciÃ³n (si disponibles)
  - InformaciÃ³n de premios y nominaciones

================================================================================
Proyecto completado exitosamente - Noviembre 2025
Autor: Victor W. Key
================================================================================
